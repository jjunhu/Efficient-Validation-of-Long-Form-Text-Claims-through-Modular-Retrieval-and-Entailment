{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# Function to generate URLs\n",
    "def generate_urls():\n",
    "    urls_with_book_nums = []\n",
    "    for y in ['6b', '175b']:\n",
    "        for x in range(268):  # Range is from 0 to 267\n",
    "            url = f\"https://openaipublic.blob.core.windows.net/recursive-book-summ/website/data/booksum_book_trees/{y}/{x}/all.json\"\n",
    "            urls_with_book_nums.append((url, y, x))\n",
    "    return urls_with_book_nums\n",
    "\n",
    "# Function to scrape data from a URL\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model_helper import FILE_PATH\n",
    "\n",
    "l3_df = pd.read_csv(FILE_PATH) # contains level 3 information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summarized range from json data, then append all summaries from csv file/df\n",
    "level_2_data = []\n",
    "for url, model_size, book_num in generate_urls():\n",
    "    data = scrape_data(url)\n",
    "    # if there are 3 levels of summaries, then we need to append the text from depth 3 summaries\n",
    "    if len(data) == 4:\n",
    "        for level, entries in data.items():\n",
    "            if int(level) == 2:\n",
    "                for entry in entries:\n",
    "                    # ignore entries where summarize range does not exist\n",
    "                    if not entry[\"summarize_range\"]:\n",
    "                        continue\n",
    "                    l3_start_range, l3_end_range = entry[\"summarize_range\"][0], entry[\"summarize_range\"][1]\n",
    "                    chunk_index = entry[\"n_prev_summaries\"]\n",
    "                    filtered_df = l3_df[(l3_df['book_num'] == book_num) & (l3_df['model_size'] == model_size) & (l3_df[\"document_index\"] >= l3_start_range) & (l3_df[\"document_index\"] < l3_end_range)]\n",
    "                    sorted_filtered_df = filtered_df.sort_values(by='document_index')\n",
    "                    concatenated_text = \"\\n\\n\".join(sorted_filtered_df['book_text'])\n",
    "                    level_2_data.append({\n",
    "                        \"depth_2_summary\": entry[\"summary\"],\n",
    "                        \"book_text\": concatenated_text, \n",
    "                        \"model_size\": model_size,\n",
    "                        \"book_num\": book_num, \n",
    "                        \"document_index\": chunk_index\n",
    "                    })\n",
    "    # else we only care about the depth 2 summaries, and add the summaries and text normally\n",
    "    elif len(data) == 3:\n",
    "        for level, entries in data.items():\n",
    "            if int(level) == 2:\n",
    "                for entry in entries:\n",
    "                    level_2_data.append({\n",
    "                        \"depth_2_summary\": entry[\"summary\"],\n",
    "                        \"book_text\": entry[\"text\"],\n",
    "                        \"model_size\": model_size,\n",
    "                        \"book_num\": book_num, \n",
    "                        \"document_index\": chunk_index\n",
    "                    })\n",
    "        \n",
    "                    \n",
    "# Write the extracted data to a CSV file\n",
    "with open('data/l2_extracted_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['depth_2_summary', 'book_text', \"model_size\", \"book_num\", \"document_index\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for entry in level_2_data:\n",
    "        writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_df = pd.read_csv(\"data/l2_extracted_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summarized range from json data, then append all summaries from csv file/df\n",
    "level_1_data = []\n",
    "for url, model_size, book_num in generate_urls():\n",
    "    data = scrape_data(url)\n",
    "    # if there is only depth_0 and depth_1 summary, then we get book text from depth_1 summary; else we get from depth_2 \n",
    "    if len(data) > 2:\n",
    "        for level, entries in data.items():\n",
    "            if int(level) == 1:\n",
    "                for entry in entries:\n",
    "                    # ignore entries where summarize range does not exist\n",
    "                    if not entry[\"summarize_range\"]:\n",
    "                        continue\n",
    "                    l2_start_range, l2_end_range = entry[\"summarize_range\"][0], entry[\"summarize_range\"][1]\n",
    "                    chunk_index = entry[\"n_prev_summaries\"]\n",
    "                    filtered_df = l2_df[(l2_df['book_num'] == book_num) & (l2_df['model_size'] == model_size) & (l2_df[\"document_index\"] >= l2_start_range) & (l2_df[\"document_index\"] < l2_end_range)]\n",
    "                    sorted_filtered_df = filtered_df.sort_values(by='document_index')\n",
    "                    concatenated_text = \"\\n\\n\".join(sorted_filtered_df['book_text'])\n",
    "                    level_1_data.append({\n",
    "                        \"depth_1_summary\": entry[\"summary\"],\n",
    "                        \"book_text\": concatenated_text, \n",
    "                        \"model_size\": model_size,\n",
    "                        \"book_num\": book_num, \n",
    "                        \"document_index\": chunk_index\n",
    "                    })\n",
    "    # else we only care about the depth 2 summaries, and add the summaries and text normally\n",
    "    elif len(data) == 2:\n",
    "        for level, entries in data.items():\n",
    "            if int(level) == 1:\n",
    "                for entry in entries:\n",
    "                    level_1_data.append({\n",
    "                        \"depth_1_summary\": entry[\"summary\"],\n",
    "                        \"book_text\": entry[\"text\"],\n",
    "                        \"model_size\": model_size,\n",
    "                        \"book_num\": book_num, \n",
    "                        \"document_index\": chunk_index\n",
    "                    })\n",
    "        \n",
    "                    \n",
    "# Write the extracted data to a CSV file\n",
    "with open('data/l1_extracted_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['depth_1_summary', 'book_text', \"model_size\", \"book_num\", \"document_index\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for entry in level_1_data:\n",
    "        writer.writerow(entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
